<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Fengyu Yang</title>
  
  <meta name="author" content="Fengyu Yang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<!-- <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>"> -->
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Fengyu Yang</name>
              </p>
              <p>Hi! This is Fengyu Yang, a senior undergraduate at University of Michigan College of Engineering majoring in Computer Science, advised by Prof. <a href="https://andrewowens.com/">Andrew Owens</a>. I also spent a wonderful summer last year at Zhejiang University advised by Prof. <a href="https://person.zju.edu.cn/en/xilics">Xi Li</a>.
              </p>

              <p>
                My undergraduate research interest lies in Computer Vision and Deep Learning, particularly in multi-modal perception, tactile sensing, continue learning and semantic segmentation.
              </p>

              <p>
                Email:fredyang at umich dot edu
              </p>

              <p>
                
                <font color="red"><strong>I am currently applying for Ph.D. in Computer Science for Fall 2023!</strong></font>
              </p>
              <p style="text-align:center">
                <a href="mailto:fredyang@umich.edu">Email</a> &nbsp/&nbsp
                <!-- <a href="data/JonBarron-CV.pdf">CV</a> &nbsp/&nbsp
                <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp -->
                <!-- <a href="https://scholar.google.com/citations?hl=en&user=jktWnL8AAAAJ">Google Scholar</a> &nbsp/&nbsp -->
                <!-- <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp/&nbsp -->
                <a href="https://github.com/fred206968/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/fred.jpg"><img style="width:80%;max-width:100%" alt="profile photo" src="images/fred.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        
        <h2>News</h2>
            <ul>
            <li><p>2022/11: One paper submitted to CVPR 2023.</p></li>
            <li><p>2022/09: "Touch and Go: Learning from Human-Collected Vision and Touch" accepted by NeurIPS 2022.</p></li>
            <li><p>2022/09: Nominated by University of Michigan for application of the CRA Outstanding Undergraduate Researcher Award.</p></li>
            <li><p>2022/07: "RBC: Rectifying the Biased Context in Continual Semantic Segmentation" accepted by ECCV 2022.</p></li>
            <li><p>2022/03: "Sparse and Complete Latent Organization for Geospatial Semantic Segmentation" accepted by CVPR 2022.</p></li>
            <li><p>2021/12: Accpet my first invitation to be a reviewer in CVPR 2022.</p></li>
            <li><p>2021/11: Two papers submitted to CVPR 2022.</p></li>
            </li>
            </ul>
        
        <br>
        
        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        
        </tbody></table> -->
        <h2>Publications</h2>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
		  
          <tr onmouseout="samurai_stop()" onmouseover="samurai_start()">
            <td style="padding:10px;width:25%;vertical-align:middle">
              <img src="images/touch-go.png" alt="touch-go" width="200px" />&nbsp;
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <strong>Touch and Go: Learning from Human-Collected Vision and Touch</strong>
              <br>
              <br>
              <strong>Fengyu Yang*</strong>,
              <a href="https://www.linkedin.com/in/chenyang-ma-66945091">Chenyang Ma*</a>, 
              <a href="https://www.linkedin.com/in/jiacheng-zhang-689b8319a">Jiacheng Zhang</a>, 
              <a href="https://jwzhi.github.io/">Jing Zhu</a>, 
              <a href="http://robotouch.ri.cmu.edu/yuanwz/">Wenzhen Yuan</a>, 
              <a href="https://andrewowens.com/">Andrew Owens</a>
              <br>
              <em>NeurIPS (Datasets and Benchmarks Track)</em>, 2022
              <br>
              <a href="https://touch-and-go.github.io/">project page</a> /
              <!-- <a href="https://www.youtube.com/watch?v=LlYuGDjXp-8">video</a> / -->
              <a href="https://openreview.net/pdf?id=ZZ3FeSSPPblo">paper</a> /
              <a href="https://drive.google.com/drive/folders/1NDasyshDCL9aaQzxjn_-Q5MBURRT360B">dataset</a>
              <p></p>
              <p>A dataset of paired vision-and-touch data collected by humans. We apply it to: 1) restyling an image to match a tactile input, 2) self-supervised representation learning, 3) multimodal video prediction.
              </p>
            </td>
          </tr>
          
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
		  
          <tr onmouseout="samurai_stop()" onmouseover="samurai_start()">
            <td style="padding:10px;width:25%;vertical-align:middle">
              <img src="images/RBC.jpg" alt="touch-go" width="200px" />&nbsp;
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              
              <strong>RBC: Rectifying the Biased Context in
                Continual Semantic Segmentation</strong>
              <br>
              <br>
              <a href="https://scholar.google.com/citations?user=F2kiw10AAAAJ&hl">Hanbin Zhao*</a>, 
              <strong>Fengyu Yang*</strong>,
              <a href="">Xinghe Fu</a>, 
              <a href="https://person.zju.edu.cn/en/xilics">Xi Li</a>
              <br>
              <em>ECCV</em>, 2022
              <br>
              <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136940054.pdf">paper</a> /
              <a href="https://github.com/sntc129/RBC">code</a>
              <p></p>
              <p>We first consider the biased context in continue semantic segmentation (CSS) and propose a context-rectified image-duplet learning scheme and a biased-context-insensitive consistency loss to tackle CSS problem.
              </p>
            </td>
          </tr>

        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
		  
          <tr onmouseout="samurai_stop()" onmouseover="samurai_start()">
            <td style="padding:10px;width:25%;vertical-align:middle">
              <img src="images/Geospatial.png" alt="touch-go" width="200px" />&nbsp;
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              
              <strong>Sparse and Complete Latent Organization for Geospatial Semantic Segmentation</strong>
              <br>
              <br>
              <strong>Fengyu Yang*</strong>,
              <a href="https://www.linkedin.com/in/chenyang-ma-66945091">Chenyang Ma*</a>, 
              <br>
              <em>CVPR</em>, 2022
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Sparse_and_Complete_Latent_Organization_for_Geospatial_Semantic_Segmentation_CVPR_2022_paper.pdf">paper</a>
              <p></p>
              <p>We propose a prototypical contrastive learning method using both foreground and background categories to tackle the large intra-class variance in geospatial semantic segmentation.
              </p>
            </td>
          </tr>

        </tbody></table>

        <h2>Honors and Awards</h2>
            <ul>
            <li>Wang Chu Chien-Wen Research Award, University of Michigan. April 2022.</li>
            <li>Henry Ford II Prize, University of Michigan. March 2022.</li>
            <li>EECS Scholar, University of Michigan. 2021-2022.</li>
            <li>James B. Angell Scholar, University of Michigan. 2021-2022.</li>
            </ul>
            <br>
        <h2>Academic Service</h2>
            <ul>
            <li><strong>Reviewer</strong>: CVPR 2022-2023, ECCV 2022, NeurIPS 2022 Track on Datasets and Benchmarks, AAAI 2023.</li>
            <li><strong>Teaching Assistant</strong>: <a href="https://www.eecs.umich.edu/courses/eecs442-ahowens/fa22/">EECS 504/442 Computer Vision.</a> </li>
            </ul>


            <br>
            <br>
            <br>
            <div align="right">Website template from <a href="https://jonbarron.info/">Jon Barron</a></div> 


</body>

</html>
